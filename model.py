from crewai import Agent, Crew, Process, Task
from crewai_tools import DirectorySearchTool
from dotenv import load_dotenv
import os
from langchain.prompts import PromptTemplate
from langchain.agents import Tool
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma

# Load environment variables
load_dotenv()
# Set API keys
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["GROQ_API_KEY"] = os.getenv("groq_api")
os.environ["HF_TOKEN"] = os.getenv("HF_API_KEY")


persist_directory = "dbAlldata"


from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Initialize the LLM with the correct API key
llm = ChatOpenAI(
    openai_api_base="https://api.groq.com/openai/v1",
    openai_api_key=os.environ["groq_api"],
    model_name="llama-3.1-8b-instant",
    temperature=0,
)
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2", model_kwargs={"device": "cuda"}
)
# Initialize the search tool with directory search
search_tool = DirectorySearchTool(
    directory="Data2",
    config=dict(
        llm=dict(
            provider="groq",
            config=dict(
                model="llama-3.1-8b-instant",
            ),
        ),
        embedder=dict(
            provider="huggingface",
            config=dict(
                model="sentence-transformers/all-MiniLM-L6-v2",
            ),
        ),
    ),
)

""" Retriever for citations """
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)

# vectordb.persist()
retriever = vectordb.as_retriever(search_kwargs={"k": 3})

# Combine Tools - Define tool use for search and summarization
tools = [
    Tool(
        name="Search",
        func=lambda query: search_tool.run(query),  # Ensure the tool is callable
        description="Useful for searching documents directory and answering questions.",
    ),
]

prompt_template = PromptTemplate(
    input_variables=["question"],
    template="""
    Question: {question}
    
    If you don't know the answer based on the information provided or retrieved, just say "I don't know."
    Don't try to make up an answer.
    """,
)


# Define the Research Agent with memory enabled and tools integrated
Retriever_Agent = Agent(
    role="Research Agent",
    goal="Search through the directory to find relevant answers.",
    backstory=(
        "You are an assistant for question-answering tasks."
        "Use the information present in the retrieved context to answer the question."
        "You have to provide a clear concise answer."
        "If you don't know the answer say I don't know."
    ),
    verbose=True,
    allow_delegation=False,
    llm=llm,  # Pass the tools to the agent
    tools=tools,
    prompt=prompt_template,
    max_iter=3,
)


hallucination_checker = Agent(
    role="Hallucination Checker",
    goal="Check the generated response for hallucinations and ensure factual accuracy.",
    backstory=(
        "You are responsible for reviewing the response generated by the Research Agent. "
        "Your task is to identify and correct any hallucinations or unsupported claims."
    ),
    verbose=True,
    allow_delegation=False,
    llm=llm,
    max_iter=1,
)

content_writer_agent = Agent(
    role="Content Writer",
    goal="Write engaging content based on the provided research or information.",
    backstory=(
        "You are a skilled writer who excels at turning raw data into captivating narratives."
        "Your task is to write clear, structured, and engaging content."
    ),
    verbose=True,
    allow_delegation=False,
    llm=llm,
    max_iter=1,
)

conclusion_agent = Agent(
    role="Conclusion Agent",
    goal="Generate a concise and well-rounded conclusion or summary based on the final output.",
    backstory=(
        "You are responsible for generating a final summary or conclusion based on the "
        "results of all previous tasks. Your job is to ensure the output is clear, "
        "concise, and comprehensively covers the key points."
    ),
    verbose=True,
    allow_delegation=False,
    llm=llm,
    max_iter=1,
)


# Define the retriever task
retriever_task = Task(
    description=(
        "Based on the user's question, extract information for the question {question} "
        "with the help of the tools. Use the Search tool to retrieve information from the directory."
    ),
    expected_output=(
        "You should answer the user's question based on the information retrieved from the directory."
        "Return a clear and concise text as a response. If you don't have the answer, return 'I don't know'."
    ),
    agent=Retriever_Agent,
)

content_writer_task = Task(
    description=(
        "Use the verified research information provided by the Research Agent."
        "Your task is to create a well-structured and engaging piece of content."
        "Focus on clarity, readability, and flow. The content should be suitable for"
        "the intended audience and the topic should be covered comprehensively."
    ),
    expected_output=(
        "A complete and engaging piece of content"
        "that is well-structured, easy to read, and aligns with the information provided."
        "The final content should be formatted and ready for publication."
    ),
    context=[retriever_task],  # Pass the previous task as context
    agent=content_writer_agent,
)

hallucination_task = Task(
    description=(
        "Review the response generated by the Research Agent and check for hallucinations."
        "Ensure that the response is factually accurate."
    ),
    expected_output=("A validated and corrected response, free of hallucinations."),
    context=[content_writer_task],
    agent=hallucination_checker,
)

# Define a more concise Conclusion Task
conclusion_task = Task(
    description=(
        "Generate a concise summary of the results from the previous tasks. "
        "The conclusion should focus on the key points and provide a brief overview. "
        "Keep the summary to one to six sentences."
    ),
    expected_output=(
        "A brief, one to six sentences summary that highlights the key takeaways from the previous tasks."
    ),
    context=[content_writer_task, hallucination_task],  # Include prior tasks as context
    agent=conclusion_agent,
)


# Define the Crew with memory enabled
rag_crew = Crew(
    agents=[
        Retriever_Agent,
        content_writer_agent,
        hallucination_checker,
        conclusion_agent,
    ],
    tasks=[retriever_task, content_writer_task, hallucination_task, conclusion_task],
    verbose=True,
    process=Process.sequential,
    memory=True,  # Enable memory for the entire crew
    embedder=dict(
        provider="huggingface",
        config=dict(
            model="sentence-transformers/all-MiniLM-L6-v2",
        ),
    ),
)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,  # Ensure this is set to True
)


def process_llm_response(llm_response):
    print("\nSources:")
    source_temp = []
    true_temp = []
    for source in llm_response["source_documents"]:
        if source.metadata["source"] not in source_temp:
            source_temp.append(source.metadata["source"])
    for source in source_temp:
        filename = os.path.basename(source)
        name_without_extension = os.path.splitext(filename)[0]
        url = name_without_extension.replace("_", "/").replace("+", ":")
        true_temp.append(url)
        print(source)
    return true_temp


def handle_query(query):
    inputs = {"question": query}

    # Call the CrewAI process with the input question
    result = rag_crew.kickoff(inputs=inputs)

    if not result or "I don't know" in result:
        return "I don't know"

    print(result)

    # Invoke the LLM for additional query handling
    llm_response = qa_chain.invoke(query)
    process_llm_response(llm_response)


from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI()


# Define a Pydantic model for the request body
class QuestionRequest(BaseModel):
    question: str


def serialize_crew_output(crew_output):

    return {
        "output": str(crew_output),
    }


@app.post("/ask")
async def ask_question(request: QuestionRequest):
    try:
        question = request.question
        if not question:
            raise HTTPException(status_code=400, detail="No question provided")

        inputs = {"question": question}
        result = rag_crew.kickoff(inputs=inputs)

        serialized_result = serialize_crew_output(result)

        llm_response = qa_chain.invoke(question)
        link = process_llm_response(llm_response)

        return {"result": serialized_result, "link": link}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn

    try:
        uvicorn.run(app, host="0.0.0.0", port=5001)
    except KeyboardInterrupt:
        print("Server shut down gracefully")
    except Exception as e:
        print(f"An error occurred: {e}")
